{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Evaluation File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script this script produces the ROC plot, as well as several other performance metrics, including the classifier scores, the log-loss for each classifier, the confusion matrix and the classification report including the f1 score. The f1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_plotting(title, y_test, y_score):\n",
    "    \n",
    "    '''\n",
    "    This function generates the ROC plot for a given model.\n",
    "    \n",
    "    Written by Jakke-Neiro\n",
    "    Last Modified by AndreiRoibu\n",
    "    \n",
    "    Args:\n",
    "        title (string): String represending the name of the model.\n",
    "        y_test (ndarray): 1D array of test dataset \n",
    "        y_score (ndarray): 1D array of model-predicted labels\n",
    "        \n",
    "    Returns:\n",
    "        ROC Plot\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    n_classes = 2\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test, y_score)\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "        \n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr[0], tpr[0], color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[0])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(X_train, X_test, y_train, y_test, classifier, predicted_train, predicted_test):\n",
    "    \n",
    "    '''\n",
    "    This function prints the results of the different classifiers,a s well as several performance metrics\n",
    "    \n",
    "    Written by AndreiRoibu\n",
    "    \n",
    "    Args:\n",
    "        X_train (ndarray): 2D array of input dataset used for training\n",
    "        X_test (ndarray): 2D array of input dataset used for testing\n",
    "        y_train (ndarray): 1D array of train labels \n",
    "        y_test (ndarray): 1D array of test labels \n",
    "        classifier\n",
    "        predicted_train (ndarray): 1D array of model-predicted labels for the train dataset \n",
    "        predicted_test (ndarray): 1D array of model-predicted labels for the test dataset\n",
    "        \n",
    "    Returns:\n",
    "        ROC Plot\n",
    "        \n",
    "    '''\n",
    "        \n",
    "    print(\"Training set score: %f\" % classifier.score(X_train, y_train))\n",
    "    print(\"Training log-loss: %f\" % log_loss(X_train, y_train))\n",
    "    print(confusion_matrix(y_train,predicted_train))\n",
    "    print(classification_report(y_train,predicted_train))\n",
    "    \n",
    "    print(\"Test set score: %f\" % classifier.score(X_test, y_test))\n",
    "    print(\"Test log-loss: %f\" % log_loss(X_test, y_test))\n",
    "    print(confusion_matrix(y_test,predicted_test))\n",
    "    print(classification_report(y_test,predicted_test))\n",
    "\n",
    "    ROC_plotting(\"ROC\",y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
